{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate machine learning Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_full = pd.read_csv('data/intermediate_train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('data/intermediate_test.csv', index_col='Id')\n",
    "\n",
    "# Obtain target and predictors\n",
    "y = X_full.SalePrice\n",
    "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "X = X_full[features].copy()\n",
    "X_test = X_test_full[features].copy()\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>11694</td>\n",
       "      <td>2007</td>\n",
       "      <td>1828</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>6600</td>\n",
       "      <td>1962</td>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>13360</td>\n",
       "      <td>1921</td>\n",
       "      <td>964</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>13265</td>\n",
       "      <td>2002</td>\n",
       "      <td>1689</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>13704</td>\n",
       "      <td>2001</td>\n",
       "      <td>1541</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LotArea  YearBuilt  1stFlrSF  2ndFlrSF  FullBath  BedroomAbvGr  \\\n",
       "Id                                                                    \n",
       "619    11694       2007      1828         0         2             3   \n",
       "871     6600       1962       894         0         1             2   \n",
       "93     13360       1921       964         0         1             2   \n",
       "818    13265       2002      1689         0         2             3   \n",
       "303    13704       2001      1541         0         2             3   \n",
       "\n",
       "     TotRmsAbvGrd  \n",
       "Id                 \n",
       "619             9  \n",
       "871             5  \n",
       "93              5  \n",
       "818             7  \n",
       "303             6  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the models\n",
    "model_1 = RandomForestRegressor(n_estimators=50, random_state=0)\n",
    "model_2 = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model_3 = RandomForestRegressor(n_estimators=100, criterion='absolute_error', random_state=0)\n",
    "model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\n",
    "model_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n",
    "\n",
    "models = [model_1, model_2, model_3, model_4, model_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 MAE: 24015\n",
      "Model 2 MAE: 23740\n",
      "Model 3 MAE: 23528\n",
      "Model 4 MAE: 23996\n",
      "Model 5 MAE: 23706\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different models\n",
    "def score_model(model, X_t=X_train, X_v=X_valid, y_t=y_train, y_v=y_valid):\n",
    "    model.fit(X_t, y_t)\n",
    "    preds = model.predict(X_v)\n",
    "    return mean_absolute_error(y_v, preds)\n",
    "\n",
    "for i in range(0, len(models)):\n",
    "    mae = score_model(models[i])\n",
    "    print(\"Model %d MAE: %d\" % (i+1, mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('data/melb_data.csv')\n",
    "\n",
    "# Select target\n",
    "y = data.Price\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "melb_predictors = data.drop(['Price'], axis=1)\n",
    "X = melb_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=10, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop columns with missing values):\n",
      "183550.22137772635\n"
     ]
    }
   ],
   "source": [
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any()]\n",
    "\n",
    "# Drop columns in training and validation data\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n",
    "\n",
    "print(\"MAE from Approach 1 (Drop columns with missing values):\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using simple imputer on columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Imputation):\n",
      "178166.46269899711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "print(\"MAE from Approach 2 (Imputation):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using extension to imputation on columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 3 (An Extension to Imputation):\n",
      "178927.503183954\n"
     ]
    }
   ],
   "source": [
    "# Make copy to avoid changing original data (when imputing)\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns\n",
    "\n",
    "print(\"MAE from Approach 3 (An Extension to Imputation):\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10864, 12)\n",
      "Car               49\n",
      "BuildingArea    5156\n",
      "YearBuilt       4307\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Shape of training data (num_rows, num_columns)\n",
    "print(X_train.shape)\n",
    "\n",
    "# Number of missing values in each column of training data\n",
    "missing_val_count_by_column = (X_train.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_full = pd.read_csv('data/intermediate_train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('data/intermediate_test.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "X = X_full.select_dtypes(exclude=['object'])\n",
    "X_test = X_test_full.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>20</td>\n",
       "      <td>90.0</td>\n",
       "      <td>11694</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>2007</td>\n",
       "      <td>452.0</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>774</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>20</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6600</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1962</td>\n",
       "      <td>1962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>308</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>30</td>\n",
       "      <td>80.0</td>\n",
       "      <td>13360</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1921</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>713</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>432</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13265</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2002</td>\n",
       "      <td>2002</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1218</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>857</td>\n",
       "      <td>150</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>20</td>\n",
       "      <td>118.0</td>\n",
       "      <td>13704</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>843</td>\n",
       "      <td>468</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "Id                                                                           \n",
       "619          20         90.0    11694            9            5       2007   \n",
       "871          20         60.0     6600            5            5       1962   \n",
       "93           30         80.0    13360            5            7       1921   \n",
       "818          20          NaN    13265            8            5       2002   \n",
       "303          20        118.0    13704            7            5       2001   \n",
       "\n",
       "     YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  GarageArea  \\\n",
       "Id                                                     ...               \n",
       "619          2007       452.0          48           0  ...         774   \n",
       "871          1962         0.0           0           0  ...         308   \n",
       "93           2006         0.0         713           0  ...         432   \n",
       "818          2002       148.0        1218           0  ...         857   \n",
       "303          2002       150.0           0           0  ...         843   \n",
       "\n",
       "     WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  \\\n",
       "Id                                                                              \n",
       "619           0          108              0          0          260         0   \n",
       "871           0            0              0          0            0         0   \n",
       "93            0            0             44          0            0         0   \n",
       "818         150           59              0          0            0         0   \n",
       "303         468           81              0          0            0         0   \n",
       "\n",
       "     MiscVal  MoSold  YrSold  \n",
       "Id                            \n",
       "619        0       7    2007  \n",
       "871        0       8    2009  \n",
       "93         0       8    2009  \n",
       "818        0       7    2008  \n",
       "303        0       1    2006  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 36)\n",
      "LotFrontage    212\n",
      "MasVnrArea       6\n",
      "GarageYrBlt     58\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Shape of training data (num_rows, num_columns)\n",
    "print(X_train.shape)\n",
    "\n",
    "# Number of missing values in each column of training data\n",
    "missing_val_count_by_column = (X_train.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1168.000000</td>\n",
       "      <td>956.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1162.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>56.605308</td>\n",
       "      <td>69.614017</td>\n",
       "      <td>10589.672945</td>\n",
       "      <td>6.086473</td>\n",
       "      <td>5.572774</td>\n",
       "      <td>1970.890411</td>\n",
       "      <td>1984.692637</td>\n",
       "      <td>103.481067</td>\n",
       "      <td>439.890411</td>\n",
       "      <td>45.571918</td>\n",
       "      <td>...</td>\n",
       "      <td>473.632705</td>\n",
       "      <td>94.498288</td>\n",
       "      <td>48.044521</td>\n",
       "      <td>23.022260</td>\n",
       "      <td>3.218322</td>\n",
       "      <td>14.528253</td>\n",
       "      <td>2.118151</td>\n",
       "      <td>50.936644</td>\n",
       "      <td>6.301370</td>\n",
       "      <td>2007.819349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>42.172322</td>\n",
       "      <td>22.946069</td>\n",
       "      <td>10704.180793</td>\n",
       "      <td>1.367472</td>\n",
       "      <td>1.116908</td>\n",
       "      <td>30.407486</td>\n",
       "      <td>20.684612</td>\n",
       "      <td>182.676225</td>\n",
       "      <td>435.106803</td>\n",
       "      <td>156.229962</td>\n",
       "      <td>...</td>\n",
       "      <td>209.442320</td>\n",
       "      <td>127.312017</td>\n",
       "      <td>68.619199</td>\n",
       "      <td>63.153093</td>\n",
       "      <td>27.916593</td>\n",
       "      <td>54.009608</td>\n",
       "      <td>36.482294</td>\n",
       "      <td>550.380636</td>\n",
       "      <td>2.725977</td>\n",
       "      <td>1.335971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7589.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1953.750000</td>\n",
       "      <td>1966.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9512.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1972.000000</td>\n",
       "      <td>1993.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>379.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>477.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11601.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>167.750000</td>\n",
       "      <td>716.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>2260.000000</td>\n",
       "      <td>1120.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1390.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>15500.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MSSubClass  LotFrontage        LotArea  OverallQual  OverallCond  \\\n",
       "count  1168.000000   956.000000    1168.000000  1168.000000  1168.000000   \n",
       "mean     56.605308    69.614017   10589.672945     6.086473     5.572774   \n",
       "std      42.172322    22.946069   10704.180793     1.367472     1.116908   \n",
       "min      20.000000    21.000000    1300.000000     1.000000     1.000000   \n",
       "25%      20.000000    59.000000    7589.500000     5.000000     5.000000   \n",
       "50%      50.000000    69.000000    9512.500000     6.000000     5.000000   \n",
       "75%      70.000000    80.000000   11601.500000     7.000000     6.000000   \n",
       "max     190.000000   313.000000  215245.000000    10.000000     9.000000   \n",
       "\n",
       "         YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1   BsmtFinSF2  ...  \\\n",
       "count  1168.000000   1168.000000  1162.000000  1168.000000  1168.000000  ...   \n",
       "mean   1970.890411   1984.692637   103.481067   439.890411    45.571918  ...   \n",
       "std      30.407486     20.684612   182.676225   435.106803   156.229962  ...   \n",
       "min    1872.000000   1950.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%    1953.750000   1966.000000     0.000000     0.000000     0.000000  ...   \n",
       "50%    1972.000000   1993.000000     0.000000   379.500000     0.000000  ...   \n",
       "75%    2000.000000   2004.000000   167.750000   716.000000     0.000000  ...   \n",
       "max    2010.000000   2010.000000  1600.000000  2260.000000  1120.000000  ...   \n",
       "\n",
       "        GarageArea   WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  \\\n",
       "count  1168.000000  1168.000000  1168.000000    1168.000000  1168.000000   \n",
       "mean    473.632705    94.498288    48.044521      23.022260     3.218322   \n",
       "std     209.442320   127.312017    68.619199      63.153093    27.916593   \n",
       "min       0.000000     0.000000     0.000000       0.000000     0.000000   \n",
       "25%     336.000000     0.000000     0.000000       0.000000     0.000000   \n",
       "50%     477.500000     0.000000    26.000000       0.000000     0.000000   \n",
       "75%     576.000000   168.000000    68.000000       0.000000     0.000000   \n",
       "max    1390.000000   736.000000   547.000000     552.000000   508.000000   \n",
       "\n",
       "       ScreenPorch     PoolArea       MiscVal       MoSold       YrSold  \n",
       "count  1168.000000  1168.000000   1168.000000  1168.000000  1168.000000  \n",
       "mean     14.528253     2.118151     50.936644     6.301370  2007.819349  \n",
       "std      54.009608    36.482294    550.380636     2.725977     1.335971  \n",
       "min       0.000000     0.000000      0.000000     1.000000  2006.000000  \n",
       "25%       0.000000     0.000000      0.000000     5.000000  2007.000000  \n",
       "50%       0.000000     0.000000      0.000000     6.000000  2008.000000  \n",
       "75%       0.000000     0.000000      0.000000     8.000000  2009.000000  \n",
       "max     480.000000   738.000000  15500.000000    12.000000  2010.000000  \n",
       "\n",
       "[8 rows x 36 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    956.000000\n",
       "mean      69.614017\n",
       "std       22.946069\n",
       "min       21.000000\n",
       "25%       59.000000\n",
       "50%       69.000000\n",
       "75%       80.000000\n",
       "max      313.000000\n",
       "Name: LotFrontage, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()['LotFrontage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X_train.LotArea, X_train.LotFrontage)\n",
    "\n",
    "plt.xlim(0, 100000)\n",
    "plt.ylim(0, 200)\n",
    " \n",
    "# To show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1168"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(X_train.LotArea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1168"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(X_train.LotFrontage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " ...]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X_train.LotFrontage.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[11694  6600 13360 ...  8930  3196 16770].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[1;32m----> 4\u001b[0m reg \u001b[38;5;241m=\u001b[39m \u001b[43mLinearRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLotArea\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLotFrontage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m reg\u001b[38;5;241m.\u001b[39mscore(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Mothling\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mothling\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_base.py:678\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    674\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[0;32m    676\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 678\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    680\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m has_sw \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "File \u001b[1;32mc:\\Users\\Mothling\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Mothling\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[1;32m-> 1146\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1162\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Mothling\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:938\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 938\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    939\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    940\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    941\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    942\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    943\u001b[0m         )\n\u001b[0;32m    945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    949\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[11694  6600 13360 ...  8930  3196 16770].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression().fit(np.array(X_train.LotArea), np.array(X_train.LotFrontage))\n",
    "\n",
    "reg.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[X_train['GarageYrBlt'].isna()][['YearBuilt', 'GarageYrBlt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any()]\n",
    "\n",
    "# Drop columns in training and validation data\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (Drop columns with missing values):\n",
      "17837.82570776256\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE (Drop columns with missing values):\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using simple imputer on columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Fill in the lines below: imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# Fill in the lines below: imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (Imputation):\n",
      "18062.894611872147\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE (Imputation):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other methods replacing GarageYrBlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_X_train = X_train\n",
    "\n",
    "copy_X_valid = X_valid\n",
    "\n",
    "copy_X_train['GarageYrBlt'] = copy_X_train['GarageYrBlt'].fillna(copy_X_train['YearBuilt'])\n",
    "\n",
    "copy_X_valid['GarageYrBlt'] = copy_X_valid['GarageYrBlt'].fillna(copy_X_valid['YearBuilt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_imputed_X_train = pd.DataFrame(my_imputer.fit_transform(copy_X_train))\n",
    "copy_imputed_X_valid = pd.DataFrame(my_imputer.transform(copy_X_valid))\n",
    "\n",
    "# Fill in the lines below: imputation removed column names; put them back\n",
    "copy_imputed_X_train.columns = copy_X_train.columns\n",
    "copy_imputed_X_valid.columns = copy_X_valid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (Imputation):\n",
      "17996.141506849315\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE (Imputation):\")\n",
    "print(score_dataset(copy_imputed_X_train, copy_imputed_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
